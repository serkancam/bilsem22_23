{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kapsamlı Derin Öğrenme Rehberi",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWmGQ8lI6s4S",
        "colab_type": "text"
      },
      "source": [
        " # 🌟 Derin Öğrenme (Deep Learning) Kavramları Özet 🌟\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Kaynaklar 📚\n",
        "**1:** [Deep Learning Specialization](https://www.deeplearning.ai/) (5 Kursun özeti)\n",
        "\n",
        "**2:** [Deep Learning Drizzle](https://github.com/kmario23/deep-learning-drizzle#balloon-reinforcement-learning-hotsprings-video_game)\n",
        "\n",
        "**3:** [ŞU KARA KUTUYU AÇALIM: Yapay Sinir Ağları](https://medium.com/deep-learning-turkiye/%C5%9Fu-kara-kutuyu-a%C3%A7alim-yapay-sinir-a%C4%9Flar%C4%B1-7b65c6a5264a)\n",
        "\n",
        "**4:** [DERİNE DAHA DERİNE: Evrişimli Sinir Ağları](https://medium.com/deep-learning-turkiye/deri%CC%87ne-daha-deri%CC%87ne-evri%C5%9Fimli-sinir-a%C4%9Flar%C4%B1-2813a2c8b2a9)\n",
        "\n",
        "**5:** [Stanford University: Convolutional Neural Networks cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)\n",
        "\n",
        "**6:** [Derin Öğrenme İçin Aktivasyon Fonksiyonlarının Karşılaştırılması](https://medium.com/deep-learning-turkiye/derin-%C3%B6%C4%9Frenme-i%C3%A7in-aktivasyon-fonksiyonlar%C4%B1n%C4%B1n-kar%C5%9F%C4%B1la%C5%9Ft%C4%B1r%C4%B1lmas%C4%B1-cee17fd1d9cd)\n",
        "\n",
        "**7:** [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "**8:** [CS 230 - Derin Öğrenme](https://stanford.edu/~shervine/teaching/cs-230//)\n",
        "\n",
        "**9:** [Deep Learning A-Z™| Python ile Derin Öğrenme](https://github.com/ayyucekizrak/Udemy_DerinOgrenmeyeGiris)\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Tt6DgOD66sI",
        "colab_type": "text"
      },
      "source": [
        "Bu çalışma araştırmalar yaparken benzerlerine rastlayıp iyileştirerek derlemeye çalıştığım ve derin öğrenme (deep learning) konusunda kısa bir özet ve bolca kaynak yönlendirmesi olan (hatta sonunda koca bir liste var) hızlıca konuya giriş yapılabilinmesi için gereklilikleri özetlemektedir. Lütfen katkı vermekten çekinmeyin 👽"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQXUG3Qk7Mvi",
        "colab_type": "text"
      },
      "source": [
        "## Bu not defterine neden ihtiyacınız olabilir? 🕵\n",
        "\n",
        "\n",
        "Derin Öğrenmede birçok parametre, hiperparametre ve konsept vardır. \n",
        "Bu alanda yeni olanlar için bazı temel konular üzerinden hızlıca geçmek için bu not defteri oluşturulmuştur. \n",
        "\n",
        "Özellikle derin öğrenme terimleriyle karşılaştığınızda ne anlama geldiğini bilmeniz cümle içinde ne anlatıldığını doğru şekilde anlamanız için kritik önem arz etmektedir. Bu yüzden bu not defteri bir sözlük gibi de kullanabileceğiniz bir kaynak olma niteliğinde. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ1AxLTg_4wC",
        "colab_type": "text"
      },
      "source": [
        "## Teşekkür 🙏\n",
        "Coursera [DeepLearning.ai](https://www.deeplearning.ai/) derslerinde notasyonu temel alarak hazırlanmıştır. Bu dersi yayınlayan ekibe çokça teşekkür ederim. \n",
        "\n",
        "** 🏅Ayrıca **Stanford Üniversitesi** [CS221 - Artificial Intelligence](https://stanford.edu/~shervine/l/tr/teaching/cs-221/cheatsheet-reflex-models), [CS229 - Machine Learning](https://stanford.edu/~shervine/l/tr/teaching/cs-229/cheatsheet-deep-learning) ve [CS230 - Deep Learning](https://stanford.edu/~shervine/l/tr/teaching/cs-230/cheatsheet-convolutional-neural-networks) derslerini birlikte çevirerek Türkçe olarak takip edilmesine katkı sağladığımız,  arkadaşlarım [Başak Buluz](https://tr.linkedin.com/in/basak-buluz-62800088) ve [Yavuz Kömeçoğlu](https://yavuzkomecoglu.github.io/)'na teşekkür ederim.'** \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zuwm_Yjp7Ycv",
        "colab_type": "text"
      },
      "source": [
        "## Notasyon Özeti 📑\n",
        "Tüm not defterini takip ederken bu notasyon tanımlarını dikkate almalısınız.\n",
        "\n",
        "notasyon | tanım\n",
        "--- | ---\n",
        "$m$ | eğitim örneklerinin sayısı\n",
        "$n_x$ | her bir eğitim örneği için özrniteliklerin sayısı \n",
        "$X$ | giriş matrisi: her sütun bir eğitim örneği \n",
        "$Y$ | çıkış matrisi: her bir sütun $X$ eğitim setinin etiketi olarak tanımlanır. Örneğin: $X[0]$ 1. eğitim örneği için $Y[0]$ etikettir\n",
        "$\\hat{Y}$ | yeni test girdileri için tahmin edilen etiket\n",
        "$Z$ |  $X$ 'in doğrusal dönüşümü\n",
        "$A$\t| $Z$ 'nin doğrusal olmayan dönüşümü, aktivasyon fonksiyonu çıkışı\n",
        "$W$ |$X$ 'deki her öznitelik için ağırlık matrisi\n",
        "$x$\t| bir eğitim örneğinin öznitelikleri\n",
        "$y$\t| bir eğitim örneğinin çıkış etiketi\n",
        "$\\hat{y}$\t| bir eğitim örneğinin tahmini çıkış etiketi\n",
        "$z$\t| $x$ 'in doğrusal dönüşümü\n",
        "$a$\t| $z$ 'nin doğrusal olmayan dönüşümü, aktivasyon fonksiyonu çıkışı\n",
        "$w$\t| $x$ için ağırlık matrisi\n",
        "$b$\t| bias matrisi\n",
        "$\\sigma$ | sigmoid fonksiyonu, $\\sigma(z)= \\frac{1}{(1 + e^{-z} )}$  ve $z$ 'nin herhangi bir değeri için çıkış değeri (0,1) aralığındadır\n",
        "$x_j^{(i)}$ | i. eğitim örneği için j öznitelik değeri \n",
        "$w_j^{(i)[k]}$ | j. gizli birimin k. katmanının i. eğitim örneği için ağırlık değeri \n",
        "$L$ | derin sinir ağındaki toplam katman sayısı (giriş katmanı hariç)\n",
        "$n^{[l]}$ | $l$ gizli katmanındaki nöron sayısı \n",
        "$X^{\\{t\\}}$, $Y^{\\{t\\}}$ | mini-küme gradyan (bayır) inişinde t. mini-küme için $X$ ve $Y$değerleri\n",
        "$J$ | tüm eğitim örnekleri sonucunda model tarafından hesaplanan maliyet fonksiyonu\n",
        "$C$\t| çok-sınıf sınıflayıcı için sınıf sayısı\n",
        "$X^{(i)<t>}$ | sıralı (dizi) modellerde, i. eğitim örneğinde t. elemanın gösterimi\n",
        "$Y^{(i)<t>}$ | sıralı (dizi) modellerde, i. eğitim örneğinde t. elemanın çıkış değerinin gösterimi\n",
        "$T_X^{(i)}$ | sıralı (dizi) modellerde, i. eğitim örneği için giriş dizisinin uzunluğunun gösterimi \n",
        "$T_Y^{(i)}$ | sıralı (dizi) modellerde, i. eğitim örneği için çıkış dizisinin uzunluğunun gösterimi\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyt39ZFv_u1-",
        "colab_type": "text"
      },
      "source": [
        "## Lojistik Bağlanım (Regresyon), Logistic Regression, Derin Sinir Ağlarının Yapı Taşı 💎\n",
        "\n",
        "Sınıflanırma için kulanılan doğrusal bir modeldir. Modelin amacı verilen girişe göre çıkış olasılıklarının tahmin etmektir. \n",
        "\n",
        "$$z= w^T x+b$$\n",
        "$$a=\\sigma(z)$$\n",
        "$$\\hat{y} = a$$\n",
        "\n",
        "Ne kadar iyi tahmin yapıldığını her bir eğitim örneği için ölçmek gerekir ve bunun için çapraz entropi kaybı hesaplanmalıdır. \n",
        "\n",
        "$$L(\\hat{y},y)= -(y log(⁡\\hat{y}) + (1-y)log(⁡\\hat{y}))$$\n",
        "\n",
        "Tüm örnekler için maliyet fonksiyonu,\n",
        "$$J= -\\frac{1}{m} \\sum_{i=1}^mL(\\hat{y}^{(i)} ,y^{(i)})$$\n",
        "Buarada $J$ optimizasyon algoritması (örneğin gradyan (bayır) iniş) tarafından kullanılır.Optimiazsyonun amacı optimum $w$ ve $b$ değerlerini bulmaktır."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CApcuF65BuPL",
        "colab_type": "text"
      },
      "source": [
        "## Sığ Sinir Ağları \n",
        "\n",
        "Lojistik regresyonda $z$ ve $a$, her eğitim örneği için tahmin elde etmek için hesaplanır. Sığ bir sinir ağında, bu işlem çıkış etiketini tahmin etmek için iki kez tekrarlanır. \n",
        "\n",
        "*Lojistik regresyon:*\n",
        "\n",
        "\n",
        "![logistic_regression_network](https://drive.google.com/uc?export=view&id=1dm9gVeDOf6FOZdaBhIHp94fkRPCP7_nN)\n",
        "\n",
        "\n",
        " *Sığ sinir ağı:*\n",
        "\n",
        "![shallow_neural_network](https://drive.google.com/uc?export=view&id=1dFX5kBsG45RLvnAyFNl80TNdIN7UGizH)\n",
        "\n",
        "  \n",
        "\n",
        "[1] ve [2] ağın katmanlarıdır. [1] Katmanı ne bir giriş ne de çıkış katmanıdır, gizli katman olarak tanımlanmaktadır. [1] Katmanı 3 gizli birime (nöron) sahiptir ve katman [2] 1 tane nörona sahiptir.\n",
        "\n",
        "Sığ sinir ağında $x$ eğitim örneği (giriş) için tahmin aşağıdaki gibi hesaplanmaktadır,\n",
        "\n",
        "\n",
        "$$z^{[1]}= w^{[1]} x+b^{[1]}$$\n",
        "$$a^{[1]} = \\sigma(z^{[1]})$$\n",
        "$$z^{[2]} = w^{[2]}a^{[1]} +b^{[2]}$$\n",
        "$$\\hat{y}=a^{[2]} =\\sigma(z^{[2]})$$\n",
        "\n",
        "Bu işlem tüm eğitim örnekleri için genelleştirilirse: $Z^{[1]}$, $Z^{[2]}$, $A^{[1]}$, $A^{[2]}$, $\\hat{Y}$.  \n",
        "Eğer işlem 2'den daha fazla gizli katmana sahip olursa artık buna **derin sinir ağı** (deep neural nets) diyoruz!!!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRTDte3UW3Dd",
        "colab_type": "text"
      },
      "source": [
        "## Aktivasyon fonksiyonları\n",
        "\n",
        "- $sigmoid$, $\\sigma(z)= \\frac{1}{1+e^{-z}}$\n",
        "  - σ(z), (0, 1) aralığında değerler alır.\n",
        "  - genellikle son katmanda ikili sınıflandırma problemleri için kullanılır.\n",
        "- $tanh(z)= \\frac{e^z  - e^{-z}}{e^z  + e^{-z}}$\n",
        "  - $tanh(z)$ (-1, 1) aralığında değerler alır.\n",
        "  -  Sigmoid'ten farklı olarak grafik 0 merkezlidir.\n",
        "- $ReLU(z) = max (0,z)$\n",
        "  - $z$ çok küçük ya da çok büyük değerlere sahip olduğunda sigmoid ve tanh öğrenmesi yavaşlar. \n",
        "  - bu yüzden sigmoid ve tanh ile karşılaştırıldığında modelin öğrenme hızı ReLU'da daha yüksektir.\n",
        "  - genellikle gizli katmanlarda kullanılmaktadır.\n",
        "- $Leaky\\ ReLU(z) = max⁡(0.01z,z)$\n",
        "\n",
        "🔎 **[Aktivasyon fonksiyonları hakkında daha detaylı bilgi ve uygulama üzerinde performanslarını değerlendirmek isterseniz bu bağlantıdan devam edebilirsiniz!](https://medium.com/deep-learning-turkiye/derin-%C3%B6%C4%9Frenme-i%C3%A7in-aktivasyon-fonksiyonlar%C4%B1n%C4%B1n-kar%C5%9F%C4%B1la%C5%9Ft%C4%B1r%C4%B1lmas%C4%B1-cee17fd1d9cd)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnlEfi4HYPzG",
        "colab_type": "text"
      },
      "source": [
        "## Derin Sinir Ağları 🔥\n",
        "\n",
        "Basitçe söylemek gerekirse, çoklu gizli katmanlara sahip bir sinir ağıdır. Katman sayısı $L$ ve her katmandaki nöron sayısı, eğitimden önce karar verilen hiperparametrelerdir.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1_qm8c14Gws-k_aR1zeBzlN2O0wvP1kgt\" height=\"250px\" alt=\"deep neural nets\" />\n",
        "</center>\n",
        "<caption>\n",
        "  <center>\n",
        "    <strong>Şekil 1: </strong>\n",
        "    4 Katmanlı bir tam bağlantılığı derin sinir ağı\n",
        "  </center>\n",
        "</caption>\n",
        "\n",
        "Yukarıdaki ağ $L = 4$, $n^{[1]} = 3$, $n^{[2]} = 4$, $n^{[3]} = 3$ ve $n^{[4]}=1$ katmanlarından oluşmaktadır. Tüm eğitim örneklerinin sonucu $\\hat{Y} = A^L$'dir.  \n",
        "$X = A^{[0]}$ Şöyle hesaplanır:\n",
        " \n",
        "\n",
        "$$Z^{[1]} = W^{[1]}  A^{[0]} + b^{[1]}$$\n",
        "\n",
        "$$A^{[1]} = g^{[1]}( Z^{[1]})$$\n",
        "\n",
        "Benzer şekilde [2], [3] ve [4] katmanları için de aynı işlemler gerçekleştirilir.\n",
        "$$\\hat{Y}= A^{[L=4]}= g^{[4]}( Z^{[4]})$$\n",
        "\n",
        "\n",
        "Burada $g^{[l]}$, $l$ katmanında kullanılan aktivasyon fonksiyonudur. `numpy` vektörleri ile uygulandığında, tüm hesaplamalar eğitim örnekleri arasında paralelleştirilir ve buna vektörleştirme (vectorized) adı verilmektedir. Vektörleştirme olmadan, sinir ağı, bir eğitim epochu (dönemi) tamamlamak için eğitim örneklerini birer birer döngülendirmek zorundadır. Bu öğrenmeyi yavaşlatır. \n",
        "\n",
        "\n",
        "Her eğitim örneği $x^{(i)}$, $\\hat{y}^{(i)}$ tahminini son katmandan elde etmek için ağdan geçirilir. Bu adım, tüm süreç boyunca **ileri yayılım** olarak adlandırılır. $\\hat{y}^{(i)}$, hata tahmini elde etmek için $J$ kullanarak $y^{(i)}$ ile karşılaştırılır. Bu hata $[L]$ 'den $[L-1]$' e$[L-2]$ 'ye geri gönderilir ve böylece $[1]$ 'e, her katmandaki $W^{[l]}$, $b^{[l]}$ 'yi ayarlamak için tekrar uygulanır, böylece bir sonraki tahmin hatası daha küçük olur. Bu hatayı geri alma aşaması, tüm süreç boyunca **geriye yayılım** olarak adlandırılır. Hata her geri yayılmasında, sistemin $W^{[l]}$, $b^{[l]}$ parametrelerinde yaptığı değişiklik miktarı öğrenme hızı, $\\alpha$ adlı bir hiperparametre tarafından yönetilir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTdp6tJvicA2",
        "colab_type": "text"
      },
      "source": [
        "### Boyutların kontrol edilmesi\n",
        "Aşağıdaki formüller size bir derin sinir ağı modeli uyarlaması yaparken [matris boyutları](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.shape.html)nızın doğruluğunu kontrol etmenizde yardımcı olur: \n",
        "\n",
        "- $w^{[l]}.shape = (n^{[l]}, n^{[l-1]})$\n",
        "- $b^{[l]}.shape = (n^{[l]}, 1)$\n",
        "- $A^{[l]}.shape = Z^{[l]}.shape = (n^{[l]}, m)$\n",
        "\n",
        "*(shape: şekil, biçim, durum / numpy fonksiyonu)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTONqYzKjHfX",
        "colab_type": "text"
      },
      "source": [
        "### Hiperparametrelerin seçilmesi\n",
        "$W^{[l]}$, $b^{[l]}$sinir ağının eğitim aşamasında öğrenmeye çalıştığı parametrelerdir. \n",
        "\n",
        "Hiperparametreler, eğitim öncesi geliştirici tarafından manuel olarak ayarlanır (belli bir rastgeleliğe de sahiptir).\n",
        "\n",
        "-  öğrenme hızı alpha,  $\\alpha$  - tahminleri gerçek değerlere yaklaştırmak için parametrelerin güncellenme hızı\n",
        "- epoch sayısı - Tüm eğitim verisiyle bir kez eğitim yaptıktan sonra bir epoch tamamlanır. Bu parametre, bunun kaç kez tekrarlanması gerektiğini bulmamıza yarar.\n",
        "- gizli katmanlar, L - Derin Sinir Ağında (DNN) kaç tane gizli katman var\n",
        "- her bir gizli katmandaki nöronlar - $n^{[1]}$, $n^{[2]}$, $n^{[3]}$,…, $n^{[L]}$ değerleri için\n",
        "- aktivasyon fonksiyonları - her katmanda kullanılacak  aktivasyon fonksiyonu, $g^{[1]}$, $g^{[2]}$, $g^{[3]}$,…, $g^{[L]}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ6cdSdH8LEP",
        "colab_type": "text"
      },
      "source": [
        "## Derin Sinir Ağlarının Optimizasyonu 🏹\n",
        "\n",
        "### Video Anlatım için bağlantıları ziyaret ediniz! \n",
        "* [Yapay Öğrenme Modelleri Geliştirirken Karşılaşılan Sorunlar & Çözümleri (Part1)](https://www.youtube.com/watch?v=gbzwtZGrkrQ&t=67s)\n",
        "* [Yapay Öğrenme Modelleri Geliştirirken Karşılaşılan Sorunlar & Çözümleri (Part2)](https://www.youtube.com/watch?v=L3NJi7diDmg&t=141s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEzV5faK67hu",
        "colab_type": "text"
      },
      "source": [
        "### Veri bölme/ayırma - Aynı dağılımdaki tüm veriler ✂️\n",
        "Kullanılabilir tüm etiketlenmiş veriler\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1XNwp6CzbTE0LM1UxpPPeXAzPLZmFqpIH\" alt=\"data splitting in same distribution\" />\n",
        "\n",
        "-\t**Eğitim verisi** (Train data) – Verinin büyük bir kısmı eğitim için kullanılır\n",
        "-\t**Doğrulama/Geçerleme verisi** (Dev data) – Modelin doğruluğunu kontrol etmek ve hiğerparametrelerde ayarlama yapabilmek için kullanılır. \n",
        "-\t**Test verisi** (Test data) – Son olarak karar verilen modeli doğrulamak/ test etmek için kullanılır\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tgmguu78Gc_",
        "colab_type": "text"
      },
      "source": [
        "#### Hata Türleri ❌\n",
        "\n",
        "Şekil 2'de gösterildiği gibi, derin sinir ağları (Deep Neural Nets-DNN) test hatasının yanında eğitim ve doğrulama/geçerleme hatasına da sahip.\n",
        "\n",
        "- Önlenebilir bias – insan hatası ile eğtim hatası arasındaki fark olarak tanımlanır. Bunu azaltmanın olası çözümleri:\n",
        "    - Daha büyük bir ağı eğitmek ($L$ ya da $n^{[l]}$artırmak)\n",
        "    - Epoch sayısını artırmak\n",
        "    - Ağ mimarisini değiştirmek\n",
        "- Varyans – eğitim hatası ve doğrulama hatası arasındaki fark olarak tanımlanır. difference between training error and dev error. Bu eğitim verisinin aşırı uydurulmasında (overfitting) meydana gelir. Bunu azaltmanın olası çözümleri:\n",
        "    - Daha fazla veri ile eğitmek\n",
        "    - Regularizasyon/ Düzenleme\n",
        "    - Ağ mimarisini değiştirmek\n",
        "    \n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1jLAPA4kUOxzd6KWqrnTPelJhCPsVqRy7\" alt=\"errors when all data is from same distribution\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Şekil 2: </strong>\n",
        "    Her hatanın aralığı\n",
        "  </caption>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKy-BC7u_k_V",
        "colab_type": "text"
      },
      "source": [
        "### Veri bölme/ayırma – Farklı dağılımlara sahip veriler ✂️\n",
        "\n",
        "İdeal olarak, eğitim, doğrulama ve test kümeleri en iyi sonuç için aynı veri dağılımından olmalıdır. Ancak bazen derin öğrenme deneyi yapmak için yeterince büyük veri bulunmayabilir. Örneğin, 2 kedinizin 100 resmini sınıflandırmak için bir DNN oluşturmak için, internetten kedi resimleri üzerinde çalışmak ve 100 kedi resminizi test etmek, veri dağılımları farklı olduğundan iyi sonuçlar vermeyebilir. Bu gibi durumlarda;\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1C5033OvW72TVhpUTGGKSjS3EoK80RNyS\" height=\"75px\" alt=\"split limited data sample figure\" />\n",
        "</center>\n",
        "\n",
        "100 kedi görüntüsü 50-50 olarak ayrılabilir. (50) görüntü eğitim için internetteki görüntülerle karıştırılabilir. \n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=15g9cYPzUlqdP-qcGtyYSPeykUX2raYEO\" alt=\"how to use the split limited data figure\" />\n",
        "</center>\n",
        "\n",
        "Eğitim ve doğrulama/geçerleme verileri farklı dağılımlardadır. Bu yüzden eğitim ve doğrulama hatalarının karşılaştırılması yüksek varyans veya veri uyuşmazlığına neden olur. Bu nedenle, eğitim verileri, 50 kedi fotoğrafınızı karıştırdıktan sonra eğitim ve eğitim-doğrulama olarak ayrılmıştır.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1dYym9BHJm3T7Hb6NzdG9qKZq_pmx-E2l\" alt=\"final data after mixing all available data sources figure\" />\n",
        "</center>\n",
        "\n",
        "Eğitim ve eğitim-doğrulama setlerinin aynı dağılımdan geldiğinde, problemin temel sebebinin bias veya varyans ya da veri uyuşmazlığı olduğu anlaşılabilir.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1e-UZ8fhPa-Vh1PXG0evre3apua2rfzkF\" alt=\"range of errors when data is from different distributions figure\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Şekil 3: </strong>\n",
        "    Tüm data aynı dağılımdan gelmediğinde hata aralığı\n",
        "  </caption>\n",
        "</center>\n",
        "\n",
        "Şekil 3'te gösterildiği gibi, eğitim-doğrulama kümesi ve doğrulama kümesi farklı veri dağılımlarından olduğu için, hataları arasındaki fark veri uyumsuzluğundan kaynaklanmaktadır.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiHZj81vFAGg",
        "colab_type": "text"
      },
      "source": [
        "### Regularizasyon / Düzenlileştime\n",
        "\n",
        "Sinir ağı aşırı uydurulduğunda (yüksek varyans) model eğitim verisine uygun olduğunda, görünmeyen doğrulama kümesindeki tahminler kötü olabilir. Düzenlileştirme modeldeki (çeşitli/bazı) nöronların etkisini azaltır, böylece görünmeyen girdilerin genelleştirilmesini sağlar. Lambda $\\lambda$, L1 ve L2 algoritmalarında kullanılan düzenlileştirme miktarını kontrol eden hiperparametredir. Düzenleme için bazı algoritmalar / fikirler şöyledir:\n",
        "\n",
        "- L1 – $W$’in ceza puanının hesaplanmasında L1-norm kullanılır\n",
        "- L2 – $W$’in ceza puanının hesaplanmasında L2-norm kullanılır\n",
        "\n",
        "- Seyreltme (Dropout) – ağdaki bazı nöronları rastgele olarak sıfırlamak (yani silmek/seyreltme) modeli genelleştirmek için basit bir yöntemdir. $keep\\_prob$ bir nöronun yeniden eğitilme olasılığını hesaplayan bir hiperparametredir. Farklı katmanlar, bağlantı yoğunluğuna bağlı olarak farklı $keep\\_prob$ değerlerine sahip olabilir. \n",
        "\n",
        "- Veri artırma (Data augmentation) – giriş/eğitim görüntülerini (verilerini), rastgele kırpın, farklı renk ve boyutlara dönüştürün\n",
        "\n",
        "- Erken durdurma (Early stopping) – her epoch sonrasında doğrulama hatası artar ve eğitim hatası azalmaya devam ederse (bu aşırı uydurma olduğunun habercisi olabilir) erken durdurma uygulanması önerilir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WC8LRtMFvqW",
        "colab_type": "text"
      },
      "source": [
        "### Normalizasyon 〰️\n",
        "\n",
        "Daha hızlı öğrenme sağlamak için giriş özelliklerini değişken aralıklar ile normalleştirmek sıkça başvurulan bir yöntemdir. Normalizasyon, tüm eğitim örnekleri için $\\mu=0$ ve $\\sigma^2=1$ değerini ayarlar.\n",
        "\n",
        "- Küme normalizasyon (Batch normalization) – girişleri normalleştirme fikri tüm katmanlara genişletilir. Aktivasyon fonksiyonunu uygulamadan önce $z^{[l]}$ normalleştirilir. Bu konşulda parametreler şu şekilde hesaplanır:\n",
        "\n",
        "\n",
        "$$X \\xrightarrow{W^{[1]}, b^{[1]}} Z^{[1]} \\xrightarrow{\\beta^{[1]},\\gamma^{[1]}} \\tilde{Z}^{[1]} \\to a^{[1]} = g( \\tilde{Z}^{[1]}) \\xrightarrow{W^{[2]}, b^{[2]}} Z^{[2]}…$$\n",
        "\n",
        "> $\\tilde{Z}^{[1]}$, $Z^{[1]}$'in normalize edilmiş halidir. $\\beta^{[1]}$ ve $\\gamma^{[1]}$ parametrelerinden faydalanarak hesaplanmaktadır.\n",
        "\n",
        ">Tıpkı $W^{[l]}$ ve $b^{[l]}$ eğitim sırasında öğrenilen parametreler olduğu gibi, $\\beta^{[l]}$ ve $\\gamma^{[l]}$ de aynı şekilde öğrenilir.\n",
        "\n",
        "> Mini-küme gradyan iniş ile, eğitim sırasında kümeler arası $\\mu$ ve $\\sigma^2$ üssel ağırlıklı ortalamalar hesaplanır ve kaydedilir. Bunlar, çıkarım süresinde $\\tilde{Z}^{[l]\\{t\\}}$ değerini hesaplamak için kullanılır."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrc6yyA3J42n",
        "colab_type": "text"
      },
      "source": [
        "### Daha hızlı ve iyi bir eğitim\n",
        "\n",
        "- Mini-küme gradyan iniş - eğer eğitim seti büyükse, modeller daha iyi öğrenir, ancak her epoch daha uzun sürer. Mini-küme gradyan inişinde, girişler kümeler halinde ayrılır ve bir mini-küme üzerinde eğitimden sonra bir gradyan iniş adımı atılır. Mini-küme boyutu, hem vektörizasyon hem de daha hızlı adımlardan yararlanmak için genellikle 1 ila $m$ arasında seçilir. Tipik küme büyükleri 64, 128, 256 veya 512 eğitim örneğidir, öyle ki her bir mini-küme CPU / GPU hafızasına sığdırılmış olur.\n",
        "\n",
        "- Momentum ile gradyan iniş - mini-küme gradyan iniş, optimum seviyeye ulaşmayı yavaşlatabilen osiloasyonlara/salınımlara neden olur. Momentum bu sorunu, hareketli/kayan ortalama yöntemi ile etkileyici bir şekilde optimum değere daha hızlı ulaşmasını sağlayarak çözer. Momentum $\\beta$, kayan pencere boyutu $\\approx \\frac{1}{1- β}$ ile kontrol edilir.\n",
        "\n",
        "- RMS Prop - Minimumdan uzaktayken daha uzun boyutlarda adımlar alarak ve minimuma yaklaştıkça daha küçük boyutlarda adımlar alarak gradyan iniş algoritmasını minimuma doğru yönlendirir. Bu Optimizasyon için hiperparametreler $\\beta_2$ ve $\\epsilon$ olarak tanımlanır.  $\\epsilon$ çok önemli değildir ve sadece sıfır hatayla bölünmeyi önlemek için eklenir ve genellikle $10^{-8}$ olarak ayarlanır.\n",
        "\n",
        "- Adam – gradyan iniş, momentum ve RMS Prop fikirlerinin birleşiminden meydana gelen bir başka optimizasyon yöntemidir. Hiperparametre olarak $\\beta$, $\\beta_2$  and $\\epsilon$ kullanılır.\n",
        "\n",
        "- Öğrenme hızı azalması - mini-küme gradyan inişi minimum seviyeye osilasyon/salınımlar ekler. Öğrenme hızına bir bozulma katsayısı eklemek daha iyi yakınsamasını sağlar. Böylece $\\alpha$ artık bir sabit değildir ve şöyle hesaplanır:\n",
        "\n",
        "$$\\alpha=\\frac{1}{(1 + bozulma\\_oranı \\times epoch\\_sayısı) \\times \\alpha_0}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5OheTD8L82x",
        "colab_type": "text"
      },
      "source": [
        "### Hiperparametrelerin ayarlanması 🔧 🔨\n",
        "\n",
        "Eğitimden önce ayarlanan birçok hiperparametre olduğundan, hepsinin eşit derecede önemli olmadığını anlamak önemlidir. Örneğin,  $\\alpha$, $\\lambda$'dan daha önemlidir, bu nedenle önce $\\alpha$ ayarlamasının yapılması daha iyidir. Hiperparametreyi ayarlamak için bazı yaklaşımlar şöyledir:\n",
        "\n",
        "- Grid/Izgara tabanlı arama - Hiperparametre 1 ve 2 değerlerinin kombinasyonlarını içeren bir tablo oluşturun. Her kombinasyon için, en iyi kombinasyonu bulmak üzere doğrulama kümesinde değerlendirin.\n",
        "\n",
        "- Rastgele tabanlı arama - 1 ve 2 hiperparametre değerleri için rastgele kombinasyonlar seçin. Her bir kombinasyonda, en iyi kombinasyonu bulmak üzere doğrulama kümesini değerlendirin. Geniş bir değer alanında rastgele bir arama yaptıktan sonra, kaba rasgele aramanın sonuçlarını kullanarak ilgilenilen alanda daha ince ve detaylı bir arama yapılabilir. Rasgele eşik değerleri seçmeden önce hiperparametrelerin ölçeklendirilmesi önemlidir.\n",
        "\n",
        "- Panda VS Caviar yaklaşımı - Eğer model çoklu kombinasyonların test edilemeyeceği kadar karmaşıksa, en iyi fikir $J$'nin zamanla nasıl değiştiğini ve çalışma süresinde hiperparametre değerlerini nasıl değiştirdiğini izleyerek en iyi değeri bulmaktır.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAGhT4dLMgKo",
        "colab_type": "text"
      },
      "source": [
        "### Çoklu Sınıflandırma 🍎 🍐\n",
        "\n",
        "\n",
        "Softmax katmanı, $C$ sayıda sınıfı sınıflandırmak için son katman olarak kullanılır. Son katman olan $L$'den gelen aktivasyonlar şu şekilde hesaplanır:\n",
        "\n",
        "$t_i = (e^{z_i})^{[L]}$ değerleri için\n",
        "$$a_i^{[L]} = \\frac{t_i}{\\sum_{j=1}^C t_i}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC3vGlRyNkNl",
        "colab_type": "text"
      },
      "source": [
        "### Transfer Öğrenme 🙇\n",
        "\n",
        "Bir modelde öğrenilen parametreleri bir başka model için kullanılmasıdır. Orijinal eğitimli ağdaki son birkaç katmanı değiştirerek yeni problem için kullanılmasıdır. Yeni katmanlar daha sonra ilgilenilen yeni veri kümesi kullanılarak eğitilebilir. Bu, genellikle mevcut bir modelin ilk katmanları tarafından tanımlanan özellikler başka bir görev için yeniden kullanılabildiğinde uygulanabilir. Böylece işlem yükü konusunda da optimizasyon yapılmış olur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTtgXHRKNxPT",
        "colab_type": "text"
      },
      "source": [
        "## Evrişimli Sinir Ağları (Convolutional Neural Networks - CNN) 📸 🎥\n",
        "\n",
        "Evrişimli sinir ağları, derin sinir ağlarının bilgisayarlı görme konusunda spesifik olarak kullanılan bir alt konusudur. Bir derin sinir ağının, $X$'in özelliklerini elle ayarlamaya gerek kalmadan (öznitelik çıkarma - feature extraction) tanımlaması beklenir. Bu nedenle, bilgisayarlı görü görevlerinde $X$ olarak genellikle görüntüler, videolar kullanılır. Öznitelik  mühendisliği yapılmadan, görüntü ağa olduğu gibi iletilirse, öğrenilecek parametre sayısı görüntünün çözünürlüğüne bağlı olarak oldukça yüksek olabilir. Örneğin, giriş görüntüsü (genişlik, yükseklik, RGB kanalları) = (1000, 1000, 3) boyutlu ise, tam olarak (Şekil 1'de gösterildiği gibi)  $n^{[1]} = 1000$ olan bir katmana bağlanması, $W$ anlamına gelir. $W.shape = (1000, 3\\times10^6)$, yani 3 milyar parametre hesaplanacağı anlamına gelir. Bu kadar çok parametrenin eğitimi, çok sayıda eğitim verisi gerektirir ve bu nedenle derin sinir ağından gelen mevcut öngörüler bilgisayarla görme uygulamaları için kullanılmaz. Bu nedenle, Evrişimli Sinir Ağları (CNN) adı verilen yeni bir sınıf geliştirilmiştir.\n",
        "\n",
        "Bir derin sinir ağında daha önceki katmanların kenarlar gibi basit öznitelikleri/özellikleri tanımladığı ve daha sonrakilerinin belirli bir görüntüde daha karmaşık öznitelikleri ve örüntüleri tespit ettiği bilinmektedir. Matematikteki evrişim $\\ast$ operatörü, yukarıdaki problemleri çözer - hem daha önceki katmanlardaki kenarları tanımlar hem de tamamen bağlı bir derin sinir ağından daha az parametre gerektirir.\n",
        "\n",
        "\n",
        "---\n",
        "### Adım adım bir evrişimli sinir ağı tasarlamak isterseniz bağlantıyı ziyaret ediniz! 💻\n",
        "* **[Adım Adım Evrişimli Sinir Ağı Modeli Oluşturma](https://github.com/ayyucekizrak/Udemy_DerinOgrenmeyeGiris/blob/master/Evrisimli_Sinir_Aglari/EvrisimliSinirAgi_AdimAdim.ipynb)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7bmAD4VPT5s",
        "colab_type": "text"
      },
      "source": [
        "### Evrişim işlemi nasıl yapılır?\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1Lp2Flx0Ad--sTRzWAtVmQEIDaaEscP95\" alt=\"convolution operator animation\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Şekil 4:</strong> Evrişim işlemi\n",
        "  </caption>\n",
        "  <p>\n",
        "    <small>\n",
        "      <strong>Source:</strong> Kursun içindeki kodlama örneği “Convolution model - Step by Step - v2 (Adım adım evrişim modeli)” https://www.coursera.org/learn/convolutional-neural-networks/\n",
        "    </small>\n",
        "  </p>\n",
        "</center>\n",
        "\n",
        "- Giriş katmanındaki kanalların sayısı (3. boyut) evrişim filtresindeki boyut sayısıyla eşleşmelidir.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIDNJPYERINP",
        "colab_type": "text"
      },
      "source": [
        "### Doldurma / Piksel Ekleme\n",
        "\n",
        "$\\ast$  çalışma şekli nedeniyle, keanr bölgelerdeki matris değerlerinin (pikseller olabilir) hesaplamaya katkısı daha ortalardaki değerlerden daha azdır. Bu problemi çözmek için dış kenarlara değerler eklenir. Buna doldurma ya da piksel ekleme denir $p$ ile gösterilir. İki şekilde yapılabilir. Ya komşusu olan değer devamına eklenir ya da 0 değerleri ile doldurulabilir. \n",
        "\n",
        "-  Sıfır (Same) $\\implies p = 0$\n",
        "\n",
        "- Var olan ile (Valid) $f$ evrişim yapılan filtrenin boyutu için $p = \\frac{f-1}{2}$ \n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1tyfeyh1ewNknatMgGS1q-aVkpXQ9WXH4\" alt=\"padding\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Şekil 4:</strong> Doldurma / Piksel Ekleme\n",
        "  </caption>\n",
        "  <p>\n",
        "    <small>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R67_XjEOR9ah",
        "colab_type": "text"
      },
      "source": [
        "### Bir evrişim işlemindeki boyut hesapları\n",
        "\n",
        "$$(n, n, \\#kanal sayısı) \\ast (f, f, \\#kanal sayısı) \\to (\\lfloor \\frac{n + 2p - f}{s + 1} \\rfloor,\\lfloor \\frac{n + 2p - f}{s + 1} \\rfloor, \\#filtre sayısı)$$\n",
        "\n",
        "$n$ = giriş katmanı boyutu / görüntü için\n",
        "\n",
        "> $f$ = evrişim filtresi boyutu\n",
        "\n",
        "> $p$ = giriş katmanına yapılan ekleme miktarı\n",
        "\n",
        "> $s$ = giriş katmanında evrişimişlemi yapılırken işlemler arası adım kaydırma sayısı\n",
        "\n",
        "> $\\# filters$ = giriş katmanında evrişim için kullanılan filtre sayısı\n",
        "\n",
        "Şekil 4 için: $n = 5, \\# kanal sayısı = 1, f = 3, p = 0, s = 1, \\# filtre sayısı = 1$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbYYOyJfVglB",
        "colab_type": "text"
      },
      "source": [
        "### Ortaklama\n",
        "\n",
        "Girişin yüksekliğini ve genişliğini küçültmek için kullanılan, $\\ast$ gibi başka bir işlem türüdür. Aynı $\\ast$ gibi, ortaklama katmanları da girdi boyunca uzanan filtrelerdir. Ancak, öğrenecek herhangi bir parametresi yoktur.\n",
        "\n",
        "- Maksimum Ortaklama - Girişe uygulanan filtrenin her konumunda maksimum değeri seçilir ve dışarı aktarılır.\n",
        "\n",
        "- Ortalama Ortaklama - Girişe uygulanan filtrenin her konumundaki ortalama değer hesaplanır ve dışarı aktarılır.\n",
        "https://drive.google.com/open?id=\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1pl7drPDom0YtwyPGIK-WGdtVhTbL8YTv\" alt=\"max pooling\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Ortalama Ortaklama</strong> \n",
        "  </caption>\n",
        " <center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1yI58nXx5BJboRNuwdNt2D2qpwEDiVFB4\" alt=\"avgerage pooling\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Maksimum Ortaklama</strong> \n",
        "  </caption>\n",
        "  \n",
        "    \n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyJQQ50JcETB",
        "colab_type": "text"
      },
      "source": [
        "## Sıralı / Dizi Modeller 🗣\n",
        "\n",
        "Dizi veya dizi hale dönğştürülebilen girdilerin modellenmesi için kullanılan bir başka derin sinir ağı alt konusudur. Örneğin, bir kelime dizisi bir cümle, hava basıncı değerleri dizisi (zamanla) bir ses / müzik vb. bunlar dizi tipi verilerdir ve bu tip modellerle işlenmektedir.\n",
        "\n",
        "### Doğal Dil İşleme (Natural Language Processing - NLP 💬 💭\n",
        "\n",
        "Dizi modelleme teknikleri, Doğal (İnsan) dili işlemek için yaygın olarak kullanılmaktadır. Sinir ağları sayı matrisleri üzerinde çalıştığından (İngilizce) kelimeleri sayılara kodlamanın basit bir yolu onları one-hot encode kodlamaktır. Bunu yapmak için, İngilizce sözlükte 10000 kelime olduğunu varsayarsak, her kelimeye (10000, 1) arasında benzersiz bir rasgele sayı atanır. O zaman, eğer 'Aaron' kelimesi 3 numaraya atanmışsa, one-hot kodlanmış matrisi,\n",
        "\n",
        "<center>\n",
        "  $\n",
        "  \\begin{bmatrix}\n",
        "    0 \\\\\n",
        "    0 \\\\\n",
        "    1 \\\\\n",
        "    0 \\\\\n",
        "    . \\\\\n",
        "    . \\\\\n",
        "    . \\\\\n",
        "    0 \\\\\n",
        "  \\end{bmatrix}_{10000\\times 1}\n",
        "  $\n",
        "</center>\n",
        "\n",
        "\n",
        "Her sözcük one-hot temsiline dönüştürüldükten sonra, cümle, Şekil 5'te gösterildiği gibi bir Yineleyen (Özyineli) Sinir Ağına (Recurrent Neural Networks - RNN) verilir. Verilen bir cümle için, \"Aaron her gün okula bisikletle gider\", $x^{<1>}$  'Aaron' un one-hot kodlaması,  'her gün'ün one-hot kodlaması $x^{<2>}$ olur. Böyle devam eder tüm cümle için.  Eğer RNN'nin amacı cümle içindeki her kelimenin konuşma bölümlerini bulmaksa, o zaman $y^{<1>}$ 'Aaron' için konuşmanın bir parçası olacaktır, $y^{<2>}$ 'her gün' için konuşmanın bir parçası olacaktır. Bu görevin eğitim verileri, X = ingilizce cümleler içindeki one-hot kodlanmış ingilizce kelimeler ve Y = her cümle için her kelimenin konuşma içindeki bölümleri olacaktır.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1DO4NiO_MnorVlCqX_nUdjsoRNCNbnGbc\" alt=\"a recurrent neural network\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Şekil 5:</strong> Dizi / Sıralı Model\n",
        "  </caption>\n",
        "</center>\n",
        "\n",
        "---\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1QTnfyewDMEm6HhTLNgzTe1nJhVTKC9KU\" alt=\"a recurrent neural network\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Şekil 5:</strong> Yinelenen (Özyinelemeli) Sinir Ağı\n",
        "  </caption>\n",
        "</center>\n",
        "\n",
        "### Neden sıradan bir derin sinir ağı kullanmak yerine RNN kullanmalıyız?\n",
        "\n",
        "CNN'lerin bilgisayar görme görevleri için uygun olduğu gibi, RNN'ler de zamanla değişme özelliği olan yapıya sahip görevler için tasarlanmıştır.\n",
        "\n",
        "* Çıktıların farklı uzunluklarda olmasına izin verin $T_X \\neq T_Y$, örneğin Makine Çevirisi (Machine Translation) görevinde çıktı cümlesinin uzunluğunun giriş cümlesine bağlı olmaktadır.\n",
        "* Farklı metin konumlarında öğrenilen öznitelikleri/özellikleri paylaşabilir (NLP'de)\n",
        "* RNN'ler de, CNN'lerde olduğu gibi DNN'lere kıyasla hesaplamak için daha az parametreye sahiptir.\n",
        "\n",
        "[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)  Andrej Karpathyblog yazısı farklı tiplerdeki RNN ve NLP modellerinden bahsetmektedir. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnQIrImRM5Xn",
        "colab_type": "text"
      },
      "source": [
        "### RNN'lerde İleri Yayılım ➡️\n",
        "\n",
        "* Genellikle: $a^{<0>} = \\vec{0}$\n",
        "* $W_{aa}, W_{ax}, W_{ya}, b_a$ and $b_y$ gradyan iniş algoritması ile öğrenilen parametreler \n",
        "* $g_1$ genellikle tanh ya da ReLU ve $g_2$ genellikle sigmoid ya da softmax\n",
        "\n",
        "için;\n",
        "\n",
        "<center>\n",
        "  <p>$a^{<t>} = g_1(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a)$</p>\n",
        "  <p>$\\hat{y}^{<t>} = g_2(W_{ya}a^{<t>} + b_y)$</p>\n",
        "</center>\n",
        "\n",
        "Parametrelerin &lt;t&gt; zaman adımlarında paylaşıldığını unutmayın. Gösterim şuşekilde basitleştirilebilir:\n",
        "\n",
        "* $W_a$, $W_{aa}$ ve $W_{ax}$ matrislerinin sütunudur. \n",
        "* $[a^{<t-1>}, x^{<t>}]$ sırasıyla satırlar birleştirilir\n",
        "\n",
        "bu durumda: \n",
        "\n",
        "<center>\n",
        "$\n",
        "  \\begin{bmatrix}\n",
        "      W_{aa} & W_{ax}\n",
        "  \\end{bmatrix}\n",
        "  \\begin{bmatrix}\n",
        "      a^{<t-1>} \\\\\n",
        "      x^{<t>}\n",
        "  \\end{bmatrix} = W_{aa}a^{<t-1>} + W_{ax}x^{<t>}\n",
        "$\n",
        "</center>\n",
        "\n",
        "<center>\n",
        "  <p>$a^{<t>} = g(W_a[a^{<t-1>}, x^{<t>}] + b_a)$</p>\n",
        "  <p>$\\hat{y}^{<t>} = g(W_ya^{<t>} + b_y)$</p>\n",
        "</center>\n",
        "\n",
        "\n",
        "### RNN'lerde Geriye Yayılım ↩️ \n",
        "\n",
        "Şekil 6'da, parametrelerin ileri yayılım adımındaki kaybı hesaplamak için nasıl ileri doğru işlemlendiği ve gradyan inişinin ve geriye yayılım yoluyla parametreleri ayarlamak için kayıp/yitim $L$ türevlerini nasıl hesaplanarak güncellendiği görülebilir.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1S7febiHMC1utTh1pPSTfKn7MsTDiAluy\" alt=\"recurrent neural network computational graph\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Şekil 6:</strong> Tekrarlayan/ Özyinelemeli Sinir Ağı Hesaplama Grafiği  </caption>\n",
        "</center>\n",
        "\n",
        "<br />\n",
        "\n",
        "Denklemler incelendiğinde, ilk olarak önceki $L^{<t>}$ olduğu gibi çapraz entropi kaybı kullanılarak hesaplanır. Nihai kayıp $L$L, her $t$ zamanı için hesaplanan kayıpların toplamıdır.\n",
        "\n",
        "<center>\n",
        "  <p>$L^{<t>}(\\hat{y}^{<t>}, y^{<t>}) = - (y^{<t>}log(\\hat{y}^{<t>}) - (1-y^{<t>})log(1-\\hat{y}^{<t>}))$</p>\n",
        "  <p>$L(\\hat{y}, y) = \\sum_{t=1}^{T_y} L^{<t>}(\\hat{y}^{<t>}, y^{<t>})$</p>\n",
        "</center>\n",
        "\n",
        "Özyinelemeli sinir ağları $a$ katmanlarıındaki aktivasyon fonksiyonları ve zamana bağlı işlemlerin yapılış şekline göre sınıflandırılabilir. Bu iç birimlerdeki bağlantı ve aktivasyon işlemleri modelin bellek özelliklerini doğrudan etkilemektedir. Hatırlama ve unutma gibi işlemler bu birimlerde gerçekleştirilir.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "374uU6b4_zLs",
        "colab_type": "text"
      },
      "source": [
        "### Dil Modeli\n",
        "\n",
        "Dilbilgisel ve mantıksal olarak, \"Ram ate an apple\", \"Ram <b>an ate</b> apple\" den daha muhtemeldir. Dil modelinin amacı, ilk cümleye daha yüksek bir olasılık atamaktır. Başka bir deyişle, dil modeli\n",
        "\n",
        "<center>\n",
        "  $P(y^{<1>} = Ram,\\ y^{<2>} = ate,\\ y^{<3>} = an,\\ y^{<4>} = apple)\\ \\mathbf{>}\\ P(y^{<1>} = Ram,\\ y^{<2>} = an,\\ y^{<3>} = ate,\\ y^{<4>} = apple)$\n",
        "</center>\n",
        "\n",
        "Bu görev için bire çok (one-to-more) RNN kullanılır. $y^{<t>}$ kelime içindeki tüm kelimelerin cümle içinde $<t>$ konumunda bulunma olasılıklarıdır. $y^{<t-1>}$ girdi olarak beslenir, $x^{<t>}$ koşullu bir olasılık oluşturur $P(y^{<2>} = ate\\ |\\ y^{<1>} = Ram)$\n",
        "  \n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=181G51N_MwiB-dz9ufBsXOSrmcALILz_o\" alt=\"one-to-many RNN for modelling language\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Şekil 7:</strong> Dil modeli için bire çok (one-to-many) RNN \n",
        "  </caption>\n",
        "</center>\n",
        "\n",
        "Bunu büyük bir metin verisi üzerinde eğitmek, bir dile özgü kelimelerin sırasını modeller. Her bir $<t>$ 'de elde edilen koşullu olasılıkları kullanarak, bir cümlenin olasılığını bulabilir,\n",
        "\n",
        "<center>\n",
        "  $P(y^{<1>} = Ram,\\ y^{<2>} = ate,\\ y^{<3>} = an,\\ y^{<4>} = apple) = P(y^{<1>} = Ram) \\times P(y^{<2>} = ate\\ |\\ y^{<1>} = Ram) \\times P(y^{<3>} = an\\ |\\ y^{<1>} = Ram, \\ y^{<2>} = ate) \\times P(y^{<4>} = apple\\ |\\ y^{<1>} = Ram, \\ y^{<2>} = ate, \\ y^{<3>} = an)$\n",
        "</center>\n",
        "\n",
        "<br />\n",
        "\n",
        "$P(y^{<3>} = an\\ |\\ y^{<1>} = Ram, \\ y^{<2>} = ate)$,\n",
        "\n",
        "$3.$ nörondan elde edilir. &lt;UNK> sözlükte bulunmayan kelimeler için kullanılan özel bir belirteçtir. \n",
        "\n",
        "Benzer şekilde karakter seviyeli dil modelleri her karakterin $\\hat{y}^{<t>}$ şekilde ifade edilmesi ve &lt;UNK> olarak tanımlanma dil modelinin başarımına az da olsa katkı sağlamaktadır. Ancak karakter modelleriyle ilgili sorun, dizilerin çok daha uzun olacağı ve RNN'lerin çok uzun menzilli bağımlılıkları taşıyamayacağıdır. Ayrıca, karakter modelleri hesaplama yoğundur ve çok yaygın değildir.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb_eN0vfD8gg",
        "colab_type": "text"
      },
      "source": [
        "### Geçitlenmiş Özyinelemeli Birimler (Gated Recurrent Units - GRU)\n",
        "\n",
        "Gradyanların yok olması problemi hemen her derin sinir ağı modelinde olduğu gibi RNN'lerde de vardır. Uzun vadeli bağımlılıklar İngilizcede olduğu gibi bir çok dilde de yaygındır. Cümlenin başında kullanılan bir kelime sonundaki bir durumu belirleyebilir. Uzun vadeli bağımlılıkları sağlamak için Geçitlenmiş Özyinelemeli Birimler bellek hücresi adı verilen bir konsept sunar. \n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=18aO-ZOV9C8qfymZbcHiCHcycsNZZxMb5\" alt=\"RNN and GRU cells\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Şekil 8:</strong> RNN ve GRU hücreleri\n",
        "  </caption>\n",
        "</center>\n",
        "\n",
        "<br />\n",
        "<center>\n",
        "  <p>$\\tilde{c}^{<t>} = tanh(W_c[c^{<t-1>},\\ x^{<t>}] + b_c) $</p>\n",
        "  <p>$\\Gamma_u = \\sigma(W_u[c^{<t-1>},\\ x^{<t>}] + b_u) $</p>\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl94O2ATF28S",
        "colab_type": "text"
      },
      "source": [
        "### Uzun-Kısa Vadeli Bellekler (Long-Short Term Memory - LSTMs)\n",
        "\n",
        "LSTM, GRU yapısının özelleştirilmiş bir hali olarak tanımlanabilir. Aynı zamanda GRU LSTM'e göre daha kolay hesaplanabilirdir. LSTM yapısında geçmiş ve gelecekteki bilgiyi anlamlandıracak öznitelikler özyinelemeli olarak taşınmaktadır. Şekil 9'da basit bir LSTM yapısı gösterilmektedir. Bu modelde üç farklı noktada (giriş, hatırlama/unutma, çıkış) aktivasyon fonksiyonu kullanılmaktadır. Giriş ve çıkış katmanlarında genellikle $tanh$ fonksiyonu, hatırlama/unutma kapılarında ise her zaman $sigmoid$ fonksiyonu kullanılmaktadır. Bu yapının GRU’dan en önemli farkı, ilgililik geçitinin LSTM yapısında özelleşerek unutma $\\Gamma_f $ ve çıkış geçiti $\\Gamma_o $ olarak iki yeni denklem ile elde ediliyor olmasıdır. Unutma geçiti sayesinde geçmişten aktarılan ancak gerekli olmayan bilgilerin ağırlıklarının azaltılması sağlanmaktadır. Güncelleme geçiti ile unutmanın da etkisi ile daha efektif bir çıkış üretilmektedir.\n",
        "\n",
        "<center>\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1AQb1bvZ1eFFDKTQ5Eh0-WViSt9RH8p8C\" alt=\"RNN and GRU cells\" />\n",
        "</center>\n",
        "<center>\n",
        "  <caption>\n",
        "    <strong>Şekil 9:</strong> LSTM yapısı\n",
        "  </caption>\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTDsAEpJWEMB",
        "colab_type": "text"
      },
      "source": [
        "## Derin Öğrenme Hiperparametreleri \n",
        "\n",
        "Hiperparametre | Sembol | Ortak Değerler\t|\n",
        "--- | --- | --- | ---\n",
        "regularizasyon | $\\lambda$ | | \"ağırlık azaltılması\" ya da \"düzenlileştirme\" olarak ta bilinir\n",
        "öğrenme oranı | $\\alpha$\t| 0.01 | \n",
        "keep_prob | | 0.7 | Seyreltme (Dropout) işleminde kullanılır\n",
        "momentum | $\\beta$ | 0.9 | Adam optimizasyonunda da kullanılır \n",
        "mini-küme boyutu |  $t$\t| 64, 128, 256, 512\t| \n",
        "RMS Prop | $\\beta_2$ | 0.999 | Adam optimizasyonunda da kullanılır  \n",
        "öğrenme oranı azaltılması | | | azaltma oranı (decay_rate) olarak da bilinir\n",
        "filtre boyutu | $f^{[l]}$\t|\t| CNN'de, $l$ katmanındaki biltrenin boyutu\n",
        "adım aralığı (stride) | $s^{[l]}$ | | CNN'de, $l$ katmanındaki adım aralığı değeri\n",
        "doldurma/piksel ekleme (padding) |  $p^{[l]}$ | | CNN'de, $l$ katmanındaki doldurma/ekleme değeri\n",
        "\\# filtre sayısı | $n_c^{[l]}$ | | CNN'de, $l$ katmanında kullanılan filtre sayısı \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\t\t\n",
        "\t\t\t\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gz4dM6CB9RQ7",
        "colab_type": "text"
      },
      "source": [
        "### 🎊Deep Learning (Deep Neural Networks) Video Ders Kaynak Listesi🎊\n",
        "\n",
        "\n",
        "| No | Kurs Adı                                           | Üniversite/ Eğitmen(ler)                        | Ders WebSayfası                                               | Ders Videoları                                               | Yıl            |\n",
        "| ---- | ----------------------------------------------------- | ----------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | --------------- |\n",
        "| 1.   | **Neural Networks for Machine Learning** | Geoffrey Hinton, University of Toronto          | [Lecture-Slides](http://www.cs.toronto.edu/~hinton/coursera_slides.html) <br/> [CSC321-tijmen](https://www.cs.toronto.edu/~tijmen/csc321/) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLoRl3Ht4JOcdU872GhiYWf6jwrk_SNhz9) <br/> [UofT-mirror](https://www.cs.toronto.edu/~hinton/coursera_lectures.html) | 2012 <br/> 2014 |\n",
        "| 2.   | **Neural Networks Demystified**| Stephen Welch, Welch Labs                       | [Suppl. Code](https://github.com/stephencwelch/Neural-Networks-Demystified) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU) | 2014            |\n",
        "| 3.   | **Deep Learning at Oxford** | Nando de Freitas, Oxford University             | [Oxford-ML](http://www.cs.ox.ac.uk/teaching/courses/2014-2015/ml/) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu) | 2015            |\n",
        "| 4.   | **CS231n: CNNs for Visual Recognition** | Andrej Karpathy, Stanford University            | [CS231n](http://cs231n.stanford.edu/2015/)                   | `None`                                                       | 2015            |\n",
        "| 5.   | **CS231n: CNNs for Visual Recognition**| Andrej Karpathy, Stanford University            | [CS231n](http://cs231n.stanford.edu/2016/)                   | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC) | 2016            |\n",
        "| 6.   | **CS231n: CNNs for Visual Recognition**| Justin Johnson, Stanford University             | [CS231n](http://cs231n.stanford.edu/2017/)                   | [YouTube-Lectures](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) | 2017            |\n",
        "| 7.   | **CS224d: Deep Learning for NLP** | Richard Socher, Stanford University             | [CS224d](http://cs224d.stanford.edu)                         | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLmImxx8Char8dxWB9LRqdpCTmewaml96q) | 2015            |\n",
        "| 8.   | **CS224d: Deep Learning for NLP** | Richard Socher, Stanford University             | [CS224d](http://cs224d.stanford.edu)                         | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLlJy-eBtNFt4CSVWYqscHDdP58M3zFHIG) | 2016            |\n",
        "| 9.   | **CS224n: NLP with Deep Learning** | Richard Socher, Stanford University             | [CS224n](http://web.stanford.edu/class/cs224n/)              | [YouTube-Lectures](https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6) | 2017            |\n",
        "| 10.  | **Neural Networks** | Hugo Larochelle, Université de Sherbrooke       | [Neural-Networks](http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH) | 2016            |\n",
        "| 11.  | **Deep Learning**  | Andrew Ng, Stanford University                  | [CS230](http://cs230.stanford.edu/)                          | `None`                                                       | 2018            |\n",
        "|      |                                                       |                                                 |                                                              |                                                              |                 |\n",
        "| 12.  | **Bay Area Deep Learning** | Many legends, Stanford                          | `None`                                                       | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLrAXtmErZgOfMuxkACrYnD2fTgbzk2THW) | 2016            |\n",
        "| 13.  | **UvA Deep Learning**| Efstratios Gavves, University of Amsterdam(UvA) | [UvA-DLC](https://uvadlc.github.io/)                         | [Lecture-Videos](https://uvadlc.github.io/#lectures)         | 2018            |\n",
        "| 14.  | **Advanced Deep Learning and Reinforcement Learning** | Many legends, DeepMind                          | `None`                                                       | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs) | 2018            |\n",
        "| 15.  | **Deep Learning**  | Francois Fleuret, EPFL                          | [EE-59](https://fleuret.org/ee559/)                          | `None`                                                       | 2019            |\n",
        "| 16.  | **Deep Learning**  | Francois Fleuret, EPFL                          | [EE-59](https://fleuret.org/ee559-2018/dlc)                  | [Video-Lectures](https://fleuret.org/ee559-2018/dlc/#materials) | 2018            |\n",
        "| 17.  | **Deep Learning for Perception**| Dhruv Batra, Virginia Tech                      | [ECE-6504](https://computing.ece.vt.edu/~f15ece6504/)        | [YouTube-Lectures](https://www.youtube.com/playlist?list=PL-fZD610i7yAsfH2eLBiRDa90kL2ML0f7) | 2015            |\n",
        "| 18.  | **Introduction to Deep Learning**  | Alexander Amini, Harini Suresh and others, MIT  | [6.S191](http://introtodeeplearning.com/)                    | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI) <br/> [2017-version](https://www.youtube.com/playlist?list=PLkkuNyzb8LmxFutYuPA7B4oiMn6cjD6Rs) | 2017- 2019      |\n",
        "| 19.  | **Deep Learning for Self-Driving Cars**  | Lex Fridman, MIT                                | [6.S094](https://selfdrivingcars.mit.edu/)                   | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf) | 2017-2018       |\n",
        "| 20.  | **MIT Deep Learning** | Many Researchers,  Lex Fridman, MIT             | [6.S094, 6.S091, 6.S093](https://deeplearning.mit.edu/)      | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf) | 2019            |\n",
        "| 21.  | **Introduction to Deep Learning**| Bhiksha Raj and many others, CMU                | [11-485/785](http://deeplearning.cs.cmu.edu/)                | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLp-0K3kfddPwJBJ4Q8We-0yNQEG0fZrSa) | S2018           |\n",
        "| 22.  | **Introduction to Deep Learning**  | Bhiksha Raj and others, CMU                     | [11-485/785](http://deeplearning.cs.cmu.edu/)                | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLp-0K3kfddPyH44FP0dl0CbYprvTcfgOI)   [Recitation-Inclusive](https://www.youtube.com/playlist?list=PLLR0_ZOlbfD6KDBq93G8-guHI-J1ICeFm) | F2018           |\n",
        "| 23.  | **Deep Learning Specialization**  | Andrew Ng, Stanford                             | [DL.AI](https://www.deeplearning.ai/deep-learning-specialization/) | [YouTube-Lectures](https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w/playlists) | 2017-2018       |\n",
        "| 24.  | **Deep Learning**  | Ali Ghodsi, University of Waterloo              | [STAT-946](https://uwaterloo.ca/data-analytics/deep-learning) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE) | F2015           |\n",
        "| 25.  | **Deep Learning**  | Ali Ghodsi, University of Waterloo              | [STAT-946](https://uwaterloo.ca/data-analytics/teaching/deep-learning-2017) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLehuLRPyt1HxTolYUWeyyIoxDabDmaOSB) | F2017           |\n",
        "| 26.  | **Deep Learning** | Mitesh Khapra, IIT-Madras                       | [CS7015](https://www.cse.iitm.ac.in/~miteshk/CS7015.html)    | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT) | 2018            |\n",
        "| 27.  | **Deep Learning for AI** | UPC Barcelona                                   | [DLAI-2017](https://telecombcn-dl.github.io/2017-dlai/) <br/> [DLAI-2018](https://telecombcn-dl.github.io/2018-dlai/) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PL-5eMc3HQTBagIUjKefjcTbnXC0wXC_vd) | 2017-2018       |\n",
        "|      |                                                       |                                                 |                                                              |                                                              |                 |\n",
        "| 28.  | **Deep Learning Book** companion videos| Ian Goodfellow and others                       | [DL-book slides](https://www.deeplearningbook.org/lecture_slides.html) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLsXu9MHQGs8df5A4PzQGw-kfviylC-R9b) | 2017            |\n",
        "| 29.  | **Neural Networks**   | Grant Sanderson                                 | `None`                                                       | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) | 2017-2018       |\n",
        "| 30.  | **Deep Learning**  | Alex Bronstein and Avi Mendelson, Technion      | [CS236605](https://vistalab-technion.github.io/cs236605/info/) | [YouTube-Lectures](https://www.youtube.com/playlist?list=PLM0a6Z788YAZuqg2Ip-_dPLzEd33lZvP2) | 2018            |\n",
        "| 31.  | **Yapay Zekada Çarpıcı Yenilik: Kapsül Ağları**  | M. Ayyüce Kızrak - YAZGİG-2018/ Ankara     | [Kapsül Ağları](https://colab.research.google.com/github/ayyucekizrak/Udemy_DerinOgrenmeyeGiris/blob/master/KapsulAglari/KapsulAglari_MNIST.ipynb) | [YouTube-Lectures](https://www.youtube.com/watch?v=vkheAJgKJa4) | 2018            |\n",
        "| 32.  | **Google Colab ile Ücretsiz GPU Kullanımı**     | M. Ayyüce Kızrak - Udemy - Deep Learning A-Z™| Python ile Derin Öğrenme   | [Google Colab ile Ücretsiz GPU Kullanımı](https://colab.research.google.com/github/ayyucekizrak/Udemy_DerinOgrenmeyeGiris/blob/master/Python%20Numpy%20Giris/ilkadim.ipynb) | [YouTube-Lectures](https://www.youtube.com/watch?v=bT-T1i_Rpy8&t=3s) | 2018            |\n",
        "| 33.  | **Google Colab Free GPU Tutorial**  | Fuat Beşer - Deep Learning Turkey - Medium Blog   | [Google Colab Free GPU Tutorial](https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d) | [YouTube-Lectures](https://www.youtube.com/watch?v=bT-T1i_Rpy8&t=3s) | 2018            |\n",
        "| 34.  | **Derin Öğrenme ile Artistik Stil Transferi**   | M. Ayüce Kızrak - Udemy - Deep Learning A-Z™| Python ile Derin Öğrenme  | [Derin Öğrenme ile Artistik Stil Transferi](https://medium.com/deep-learning-turkiye/derin-%C3%B6%C4%9Frenme-ile-artistik-stil-transferi-29256789c7e8) | [YouTube-Lectures](https://www.youtube.com/watch?v=fM28e7o6CJc) | 2018            |\n",
        "| 35.  | **Derin Öğrenme İçin Aktivasyon Fonksiyonlarının Karşılaştırılması**                            | M. Ayüce Kızrak - Deep Learning Türkiye - Medium Blog  | [Derin Öğrenme İçin Aktivasyon Fonksiyonlarının Karşılaştırılması](https://medium.com/deep-learning-turkiye/derin-%C3%B6%C4%9Frenme-i%C3%A7in-aktivasyon-fonksiyonlar%C4%B1n%C4%B1n-kar%C5%9F%C4%B1la%C5%9Ft%C4%B1r%C4%B1lmas%C4%B1-cee17fd1d9cd) | [YouTube-Lectures](https://www.youtube.com/watch?v=ZMkLC-ebIqE&t=407s) | 2019            |\n",
        "|   |                             |   |  |  |          |\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}